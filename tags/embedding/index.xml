<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Embedding on Shahed Anzarus Sabab</title>
    <link>https://shahedsabab.github.io/shahed_portfolio/tags/embedding/</link>
    <description>Recent content in Embedding on Shahed Anzarus Sabab</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Apr 2017 11:02:59 -0300</lastBuildDate>
    
	<atom:link href="https://shahedsabab.github.io/shahed_portfolio/tags/embedding/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Refining Word Embeddings - Word2Vec, Glove, FastText, LexVec</title>
      <link>https://shahedsabab.github.io/shahed_portfolio/post/project-embedding/</link>
      <pubDate>Wed, 05 Apr 2017 11:02:59 -0300</pubDate>
      
      <guid>https://shahedsabab.github.io/shahed_portfolio/post/project-embedding/</guid>
      <description>The objective is to implement different word embeddings and investigate the performance on a dataset. For this task, the chosen dataset includes tweets about disasters, e.g., earthquake, wildfire. Among the tweets, some are fake and some are real. So, there are two labels such as 1:Real tweet and 0: Fake tweet. This data has been chosen so that we can see if the features extracted using different embedding techniques can retain distinguishable information to detect which tweet is fake and which one is real.</description>
    </item>
    
    <item>
      <title>Question Classification - SVM, Logistic Regression, LSTM, BERT, Doc2Vec, TF-IDF</title>
      <link>https://shahedsabab.github.io/shahed_portfolio/post/project-question/</link>
      <pubDate>Wed, 05 Apr 2017 11:01:59 -0300</pubDate>
      
      <guid>https://shahedsabab.github.io/shahed_portfolio/post/project-question/</guid>
      <description>The objective is to build a question classification model. The questions have six different categories such as: Description(DESC), Entity(ENTY), Abbreviation(ABBR), Human(HUM), Location(LOC), Numeric Value(NUM).
To investigate different approaches, the following data is used (downloaded from https://cogcomp.seas.upenn.edu/Data/QA/QC/):
Training set 5(5500 labeled questions) Test set: TREC 10 questions
Different data analyses have been performed and four different models are trained. The models are the followings:
  Tf-Idf + SVM: Tf-Idf is used for vectorizing texts and a linear model (i.</description>
    </item>
    
    <item>
      <title>News Classification - LSTM</title>
      <link>https://shahedsabab.github.io/shahed_portfolio/post/project-news/</link>
      <pubDate>Mon, 03 Apr 2017 11:00:59 -0400</pubDate>
      
      <guid>https://shahedsabab.github.io/shahed_portfolio/post/project-news/</guid>
      <description>The objective of this project is to classify news category from articles. The input data consist of 2225 news articles from the BBC news website corresponding to stories in 5 topical areas (e.g., business, entertainment, politics, sport, tech). LSTM has been applied in the classification task to categorize articles.  TensorFlow 2.0 has been used to train the model. Word embedding is used in feature generation. TSNE is used to visualize the word vectors in 2d space.</description>
    </item>
    
    <item>
      <title>Fake or Real Tweets - BERT, LSTM, TF-IDF</title>
      <link>https://shahedsabab.github.io/shahed_portfolio/post/project-fakeorreal/</link>
      <pubDate>Sun, 02 Apr 2017 11:00:59 -0400</pubDate>
      
      <guid>https://shahedsabab.github.io/shahed_portfolio/post/project-fakeorreal/</guid>
      <description>The dataset includes tweets about disasters, e.g., earthquake, wildfire. The objective is to detect if the tweet is about a real disaster vs. fake disaster. Different approaches have been performed for data cleaning and training the model. The best model can predict real vs. fake tweets with 89% accuracy using transfer learning (BERT).
The following models have been developed for training: BOW Model with Logistic Regression. (accuracy 77%) Tf-Idf with Logistic Regression.</description>
    </item>
    
  </channel>
</rss>